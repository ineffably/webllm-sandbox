# Browser LLM Overview

## How It Works

WebLLM runs large language models directly in the browser using WebGPU for hardware acceleration. Models are downloaded once and cached in IndexedDB, then executed entirely client-side with no server round-trips.

```
┌─────────────────────────────────────────────────────────┐
│                      Browser                            │
│  ┌─────────────┐    ┌─────────────┐    ┌─────────────┐ │
│  │  React UI   │───▶│  LLMService │───▶│ Web Worker  │ │
│  │  (Main)     │◀───│  (Bridge)   │◀───│ (Inference) │ │
│  └─────────────┘    └─────────────┘    └──────┬──────┘ │
│                                               │        │
│                                        ┌──────▼──────┐ │
│                                        │   WebLLM    │ │
│                                        │ MLC Engine  │ │
│                                        └──────┬──────┘ │
│                                               │        │
│                                        ┌──────▼──────┐ │
│                                        │   WebGPU    │ │
│                                        │   (GPU)     │ │
│                                        └─────────────┘ │
└─────────────────────────────────────────────────────────┘
```

---

## Core Interfaces

### ChatMessage

The fundamental unit of conversation. Follows OpenAI's chat format.

```typescript
interface ChatMessage {
  role: 'system' | 'user' | 'assistant';
  content: string;
}
```

| Role | Purpose |
|------|---------|
| `system` | Instructions/persona for the model. Sets behavior and context. |
| `user` | Human input. Questions, commands, or conversation. |
| `assistant` | Model's previous responses. Used for conversation continuity. |

### MessageStats

Performance metrics returned after each generation.

```typescript
interface MessageStats {
  promptTokens: number;      // Tokens in the input (system + history + user)
  completionTokens: number;  // Tokens generated by the model
  totalTokens: number;       // promptTokens + completionTokens
  durationMs: number;        // Total generation time
  tokensPerSecond: number;   // Generation speed (completionTokens / seconds)
}
```

---

## How Context Flows to the LLM

When you send a message, the system assembles a **messages array** that gets passed to the model:

```
┌─────────────────────────────────────────────┐
│              Messages Array                 │
├─────────────────────────────────────────────┤
│ 1. System Prompt (context/persona)          │
│    role: "system"                           │
│    content: "You are a helpful assistant.." │
├─────────────────────────────────────────────┤
│ 2. Conversation History (last N messages)   │
│    role: "user" / "assistant" alternating   │
├─────────────────────────────────────────────┤
│ 3. Current User Message                     │
│    role: "user"                             │
│    content: "What is the capital of France?"│
└─────────────────────────────────────────────┘
                    │
                    ▼
            ┌───────────────┐
            │   LLM Model   │
            └───────┬───────┘
                    │
                    ▼
            ┌───────────────┐
            │   Response    │
            │ (streamed)    │
            └───────────────┘
```

### Assembly in Code

```typescript
// In LLMService.chat()
const messages: ChatMessage[] = [
  { role: 'system', content: systemPrompt },        // Context/persona
  ...this.conversationHistory.slice(-10),           // Last 10 messages
  // Current user message is already in history at this point
];
```

---

## The System Prompt

The system prompt is your primary tool for shaping model behavior. It's always the first message in the array.

**Examples:**

```typescript
// Simple assistant
"You are a helpful AI assistant. Be concise and informative."

// Persona-based
"You are Marcus, a grumpy but knowledgeable blacksmith in a medieval village.
Speak in short, practical sentences. You know about metalworking, tools, and local gossip."

// Task-specific
"You are a code reviewer. Analyze code for bugs, security issues, and style problems.
Format your response as a bulleted list. Be direct and specific."

// Data-driven (with injected context)
`You are a customer service agent for Acme Corp.
Current user: ${userData.name}
Account status: ${userData.status}
Recent orders: ${JSON.stringify(userData.orders)}
Answer questions about their account and orders.`
```

---

## Conversation History

The model has no memory between calls. We maintain context by sending previous messages.

```typescript
class LLMService {
  private conversationHistory: ChatMessage[] = [];

  async chat(systemPrompt: string, userMessage: string) {
    // Add user message to history
    this.conversationHistory.push({
      role: 'user',
      content: userMessage
    });

    // Build full context: system + history
    const messages = [
      { role: 'system', content: systemPrompt },
      ...this.conversationHistory.slice(-10)  // Keep last 10 for context window
    ];

    // Get response
    const response = await this.engine.chat.completions.create({ messages });

    // Add assistant response to history
    this.conversationHistory.push({
      role: 'assistant',
      content: response
    });
  }

  clearHistory() {
    this.conversationHistory = [];
  }
}
```

**Why slice(-10)?** Models have limited context windows. Keeping only recent messages prevents exceeding limits and keeps responses relevant.

---

## Streaming Responses

Responses stream token-by-token for better UX:

```typescript
const response = await engine.chat.completions.create({
  messages,
  stream: true,
  stream_options: { include_usage: true }
});

let fullResponse = '';

for await (const chunk of response) {
  const delta = chunk.choices[0]?.delta?.content || '';
  fullResponse += delta;

  // Update UI in real-time
  onChunk(delta);

  // Usage stats come in final chunk
  if (chunk.usage) {
    stats = chunk.usage;
  }
}
```

---

## Generation Parameters

Control model behavior with these options:

```typescript
await engine.chat.completions.create({
  messages,
  temperature: 0.7,    // Randomness (0=deterministic, 1=creative)
  max_tokens: 512,     // Maximum response length
  stream: true,        // Enable streaming
});
```

| Parameter | Range | Effect |
|-----------|-------|--------|
| `temperature` | 0.0 - 2.0 | Lower = more focused, higher = more random |
| `max_tokens` | 1 - model max | Caps response length |
| `top_p` | 0.0 - 1.0 | Nucleus sampling threshold |

---

## Available Models

| Key | Model | Size | Best For |
|-----|-------|------|----------|
| `smol-135m` | SmolLM2-135M | ~360MB | Fast testing, simple tasks |
| `smol-360m` | SmolLM2-360M | ~376MB | Better quality, still fast |
| `qwen-0.5b` | Qwen2.5-0.5B | ~945MB | Good balance of speed/quality |
| `phi-3.5-mini` | Phi-3.5-mini | ~500MB | Best for dialogue/reasoning |

---

## Web Worker Architecture

Inference runs in a Web Worker to keep the UI responsive:

```
Main Thread                 Web Worker
┌──────────┐               ┌──────────────┐
│  React   │──messages───▶ │ MLCEngine    │
│  UI      │               │ Handler      │
│          │◀──chunks───── │              │
└──────────┘               └──────┬───────┘
                                  │
                           ┌──────▼───────┐
                           │   WebGPU     │
                           │  Inference   │
                           └──────────────┘
```

**llm.worker.ts:**
```typescript
import { WebWorkerMLCEngineHandler } from '@mlc-ai/web-llm';

const handler = new WebWorkerMLCEngineHandler();
self.onmessage = (msg) => handler.onmessage(msg);
```

**Main thread:**
```typescript
const worker = new Worker(
  new URL('./llm.worker.ts', import.meta.url),
  { type: 'module' }
);

const engine = await CreateWebWorkerMLCEngine(worker, modelId, {
  initProgressCallback: (p) => setProgress(p.progress)
});
```

---

## Token Counting

Tokens ≠ words. Rough estimate: **1 token ≈ 4 characters** or **~0.75 words**.

```
"Hello, world!" = 4 tokens
"The quick brown fox jumps over the lazy dog." = 10 tokens
```

Context windows vary by model (typically 2K-8K tokens). Monitor `promptTokens` in stats to stay within limits.
